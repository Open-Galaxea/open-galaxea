# _BASE_: 'mile.yml'

TAG: 'debug'
LOG_DIR: 'tensorboard_logs'
seed: 7

vla_path: paligemma-3b-pt-224
data_root_dir: /galaxea_dataset/galaxea/rlds/open_source
dataset_name: sample_r1_lite
run_root_dir: runs/r1_lite
adapter_tmp_dir: adapter_tmp_weights # not used

hf_token: .hf_token                   # Hugging Face token (for loading model from private repo)

# training
ckpt: experiments/checkpoints/G0_3B_base.pt
use_lora: False
lora_rank: 32 # not used
lora_dropout: 0.0 # not used
use_quantization: False
enable_bf16: True
model_param_to_bf16: False
vla_training_strategy: "vla-full-train" # training both vision and language
vlm_lr_multiplier: 1.0
weight_decay: 1e-6 # following allen
batch_size: 4
grad_accumulation_steps: 2
learning_rate: 2.5e-5 # following allen
warmup_steps: 500 # following allen
lr_scheduler_type: "cosine" # following openpi
image_aug: True
max_epochs: 4
save_steps: 5000 # 
log_steps: 100
use_torch_compile: False # As xiao suggests                                                             # True
wandb_project: r1_lite_post
wandb_entity: null
use_ema: False # True
ema: 
  update_after_step: 0                                # Step after which to update EMA weights
  power: 0.67      

# dataset
DATASET:
  window_size: ${MODEL.cond_steps} # should be ${MODEL.cond_steps}
  future_action_window_size: ${eval:'${MODEL.horizon_steps} - 1'} # should be ${MODEL.horizon_steps} - 1
  camera_views: ["head", "wrist_left", "wrist_right"]
  shuffle_buffer_size: 30000
  balance_weights: False
  use_last_action: False  # True  # 
  short_prompt: True
  aug_instruction_kwargs:
    drop_high_level_prob: 1.0
  action_proprio_normalization_type: normal
  use_pretrained_data_stats: True
  proprio_noise_std: 0.05
  image_augment_kwargs:
    head:
      random_brightness: [0.2]
      random_contrast: [0.8, 1.2]
      random_saturation: [0.8, 1.2]
      random_hue: [0.05]
      augment_order:
        - random_brightness
        - random_contrast
        - random_saturation
        - random_hue
    # if we use wrist
    wrist_left:
      random_brightness: [0.2]
      random_contrast: [0.8, 1.2]
      random_saturation: [0.8, 1.2]
      random_hue: [0.05]
      random_drop_all_image: [0.3]
      augment_order:
        - random_drop_all_image
        - random_brightness
        - random_contrast
        - random_saturation
        - random_hue
    wrist_right:
      random_brightness: [0.2]
      random_contrast: [0.8, 1.2]
      random_saturation: [0.8, 1.2]
      random_hue: [0.05]
      random_drop_all_image: [0.3]
      augment_order:
        - random_drop_all_image
        - random_brightness
        - random_contrast
        - random_saturation
        - random_hue

model_family: galaxea_zero
MODEL: 
  name: vla.galaxea_zero.GalaxeaZeroWrapper
  vla_name: "paligemma-3b-pt-224"
  load_inside: False
  pretrained_model_path: /galaxea_fulltime/pretrained_ckpts/cache/paligemma-3b-pt-224
  input_ids: True
  action_expert_only: False
  image_token_index: 257152
  vocab_size: 257216
  pad_token_id: 0
  cond_steps: 1 # len proprio
  horizon_steps: 32
  action_dim: 26 # 2 x [QPOS (6) + gripper (1)] + Torso Velocity (6) + Chassis Velocity (6)
  proprio_dim: 21  # 2 * [QPOS (6) + gripper (1)] + 4 (torso) + 3 (base vel) + last action(26)
  max_text_tokens: 55 # 55 for galaxea0002
  max_seq_len: ${eval:'${MODEL.num_input_images} * ${MODEL.vision.num_image_tokens} + ${MODEL.max_text_tokens}'} 
  max_image_text_tokens: ${MODEL.max_seq_len} # = ${max_seq_len}
  
  action_decoder_layers: 2

  flow_sampling: beta
  num_inference_steps: 10
  final_action_clip_value: null  # data normalized in [-1,1]

  action_expert_adaptive_mode: null
  num_input_images: ${eval:'${DATASET.window_size} * len(${DATASET.camera_views})'} # $DATASET.window_size * LEN($DATASET.camera_views)

  vision:
    name: vla.model.paligemma.siglip.SiglipVisionModel
    hidden_size: 1152 # siglip
    intermediate_size: 4304
    num_hidden_layers: 27
    num_attention_heads: 16
    num_channels: 3
    image_size: 224
    patch_size: 14
    layer_norm_eps: 0.000001
    attention_dropout: 0.0
    num_image_tokens: 256
    lora:
      r: ${lora_rank}
      dropout: ${lora_dropout}
    use_quantize: False
    use_lora: False
  vision_projector:
    name: vla.model.paligemma.siglip.PaliGemmaMultiModalProjector
    vision_config:
      hidden_size: 1152
      projection_dim: 2048
    lora:
      r: ${lora_rank}
      dropout: ${lora_dropout}
    use_quantize: False
    use_lora: False
  joint:
    name: vla.model.g0.joint_model.JointModel
    action_expert_adaptive_mode: null
    mixture:
      vlm:   # gemma
        hidden_size: 2048
        intermediate_size: 16384
        use_final_norm: False
        cache: True
        use_quantize: False
        use_lora: False
        adaptive_mode:  # not applicable for gemma
      proprio:
        hidden_size: 1024
        intermediate_size: 4096
        use_final_norm: True  # technically no, but sharing weights with action anyway
        cache: True
        use_quantize: False
        use_lora: False
        adaptive_mode: null
      action:
        hidden_size: 1024
        intermediate_size: 4096
        use_final_norm: True
        cache: False
        use_quantize: False
        use_lora: False
        adaptive_mode: null
    time_hidden_size: 256 # only applicable if using adaptive
    lora:
      r: ${lora_rank}
      dropout: ${lora_dropout}
    num_hidden_layers: 18
    num_attention_heads: 8
    num_key_value_heads: 1
    head_dim: 256
    max_position_embeddings: 8192
    rms_norm_eps: 0.000001
    rope_theta: 10000.0
    attention_bias: False
    attention_dropout: 0.0
    pad_token_id: 0

#################################################################################################################
# For evaluation
#################################################################################################################
EVALUATION:
  checkpoint: null     # Pretrained checkpoint path

  load_in_8bit: False                       # (For OpenVLA only) Load with 8-bit quantization
  load_in_4bit: False                       # (For OpenVLA only) Load with 4-bit quantization

  center_crop: True                         # Center crop? (if trained w/ random crop image aug)

  #################################################################################################################
  # LIBERO environment-specific parameters
  #################################################################################################################
  task_suite_name: "simpler_widowx"          # Task suite. Options: libero_spatial, libero_object, libero_goal, libero_10, libero_90
  num_steps_wait: 10                         # Number of steps to wait for objects to stabilize in sim
  num_trials_per_task: 24                    # Number of rollouts per task
  use_wrist_image: False
  #################################################################################################################
  # Utils
  #################################################################################################################
  run_id_note: None                          # Extra note to add in run ID for logging
  local_log_dir: "./experiments/logs"        # Local directory for eval logs

  use_wandb: False                            # Whether to also log results in Weights & Biases
  seed: 7                                    # Random Seed (for reproducibility)
